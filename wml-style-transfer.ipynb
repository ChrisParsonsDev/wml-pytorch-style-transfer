{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Watson Machine Learning - Style Transfer\n",
    "\n",
    "In this notebook we'll walk through connecting to the [Watson Machine Learning service](https://www.ibm.com/cloud/machine-learning) and executing a **Style Transfer** workload on the IBM Cloud **GPUs** using **PyTorch**\n",
    "\n",
    "The workflow is as follows: \n",
    "\n",
    "1. Install WGET\n",
    "2. Get the Pytorch script for Style Transfer\n",
    "3. Load our images\n",
    "4. Add credentials for writing to the IBM Cloud Object Storage bucket\n",
    "5. Connect to the Watson Machine Learning (WML) runtime to execute our job on the GPU\n",
    "6. Set up the WML job\n",
    "7. Execute the job\n",
    "8. Admire your results\n",
    "\n",
    "**PROTIP** -- *In this example we're taking the style from one image and trying to apply it to the content of another. This works best if you pick a really extreme style image. Bright primary colours are great, famous works of art are fun too. For your content image we'd suggest something with people/objects in. That way you'll end up with a distinctive result.* \n",
    "\n",
    "\n",
    "## Installing WGET\n",
    "\n",
    "The first step of the process is to install `wget` in vritual machine where your notebook is running. This will allow us to get the PyTorch code from GitHub we'll be using to perform the Style Transfer on our images.\n",
    "\n",
    "To install wget - run the code cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: wget in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages\r\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade wget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the PyTorch Code\n",
    "\n",
    "Now we've got WGET we need to head out to GitHub and grab the PyTorch script we'll be using. If you want to see the script in more detail, you can see it [here](https://github.com/ChrisParsonsDev/wml-pytorch-style-transfer/blob/master/pytorch-model/style-transfer.py).\n",
    "\n",
    "Often when we're building Machine Learning or Deep Learning models, we'll right the code in a notebook like this. Notbeooks are a great tool for visualising the data science workflow, you can see exactly what's going on, and how your data changes as you work.\n",
    "\n",
    "In this instance however, we're just interested in submitting a job to a GPU, to speed up execution time. You'd probably follow a similar workflow to this if you were trying to scale an app to production (where you're no longer trying to visualise everything) or if you wanted your code separated from your data science workflow. Data in one place, code in another. \n",
    "\n",
    "The script does a few things, here's a step by step of what's going on behind the scenes: \n",
    "\n",
    "**1. Command Line Arguments**\n",
    "\n",
    "As you'll see later in this notebook, the PyTorch script can handle a number of command line arguments. These let us specify our own style and content images, how many iterations through the photos we conduct as \"training\" and how often we want to see output. \n",
    "\n",
    "**2. Load Image Files**\n",
    "\n",
    "The next thing we need to do is load in your images. For style transfer we need two images! One, which has the \"content\" or the subject we're interested in and another with the \"style\" or pattern we'd like to copy across. The code also takes care of resizing your images so they're the same size (and you won't get any strange results). They're also made to be `400px` along their longest edge. The larger the image, the longer the training will take. \n",
    "\n",
    "**3. Download our Neural Network**\n",
    "\n",
    "In this example we're using [VGG](http://www.robots.ox.ac.uk/~vgg/), an existing neural network, to do the heavy lifting for our training. To make things simple VGG is one of the \"sample\" models included in PyTorch - we can simply load in a pretrained version and use it as we wish.\n",
    "\n",
    "**4. Select the \"Content\" and \"Style\" layers**\n",
    "\n",
    "We're pulling out a few layers of the network here. One to represent the subject of the image we want (that's our content) and 5 more to represent the \"style\" of a different image we'd like to apply to that content. \n",
    "\n",
    "**5. Perform Style Transfer**\n",
    "\n",
    "This is the real meat of the code - we're going to iterate a few 1000 times through both our style and content images and try to apply the \"Style\" to the \"Content\".\n",
    "\n",
    "**6. Save The Output**\n",
    "\n",
    "Finally, during the training process, the script will checkpoint and save the \"transferred\" images to your CoS bucket. This will let you see how things change/progress through the various epochs. It's really cool to watch how the algorithm learns, and watch your content become way more stylised. \n",
    "\n",
    "\n",
    "So - let's head out to GitHub and download the `.zip` file with the PyTorch script in it. To get the code - run the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files downloaded!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import wget\n",
    "\n",
    "#Import files \n",
    "filename = 'pytorch-model.zip'\n",
    "url = 'https://github.com/ChrisParsonsDev/wml-pytorch-style-transfer/blob/master/pytorch-model.zip?raw=true'\n",
    "\n",
    "if not os.path.isfile( filename ): wget.download( url )\n",
    "\n",
    "print('Files downloaded!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Credentials\n",
    "\n",
    "Ok, so we've got the code, now we need our images. \n",
    "\n",
    "Luckily Watson Studio makes getting the \"read\" credentials for our photos into our notebook **really simple**. You'll only need to do this once (as the credentials are the same for both images). \n",
    "\n",
    "This will allow our notebook to pass the style and content photos on to Watson Machine Learning and the GPUs for the style transfer itself.\n",
    "\n",
    "Watch out though, you'll need to make sure you rename the dictionary `image_credentials` (as with our example below). If you leave it as `credentials_1` (default). Stuff wont work and you'll get a whole bunch of errors. \n",
    "\n",
    "Follow the tutorial on [GitHub](https://github.com/ChrisParsonsDev/wml-pytorch-style-transfer) and insert your credentials into the code cell below. \n",
    "\n",
    "It should look a little like this: \n",
    "\n",
    "````\n",
    "image_credentials = {\n",
    "    'IBM_API_KEY_ID': '***',\n",
    "    'IAM_SERVICE_ID': '***',\n",
    "    'ENDPOINT': '***',\n",
    "    'IBM_AUTH_ENDPOINT': '***',\n",
    "    'BUCKET': '***',\n",
    "    'FILE': '***'\n",
    "}\n",
    "\n",
    "````\n",
    "\n",
    "Once you've done that. Run the block and move on down.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert your image credentials here and name them image_credentials\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write Credentials\n",
    "\n",
    "Where's the fun in machine learning if you can't write your output anywhere? \n",
    "\n",
    "In order to grant the Watson Machine Learning API permission to write back out to our CoS bucket we need to grab the credentials and insert them into the code block below. \n",
    "\n",
    "The [tutorial](https://github.com/ChrisParsonsDev/wml-pytorch-style-transfer) tells you how you can get your specific credentials. \n",
    "\n",
    "Make sure you call them `cos_credentials` as we did, just to keep everything working properly. Of course, you're more than welcome to go all the way through and rename the variable if you want. It just seems like a whole lot more work to us. ;) \n",
    "\n",
    "Once you've replaced the placeholder below with your details, run the code cell below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert your Cloud Object Storage credentials here!\n",
    "\n",
    "cos_credentials = {\n",
    "  \"apikey\": '***',\n",
    "  \"cos_hmac_keys\": {\n",
    "    \"access_key_id\": '***',\n",
    "    \"secret_access_key\": '***'\n",
    "  },\n",
    "  \"endpoints\": '***',\n",
    "  \"iam_apikey_description\": '***',\n",
    "  \"iam_apikey_name\": '***',\n",
    "  \"iam_role_crn\": '***',\n",
    "  \"iam_serviceid_crn\": '***',\n",
    "  \"resource_instance_id\": '***'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Watson Machine Learning\n",
    "\n",
    "Here's where the fun **really** starts. Watson Machine Learning. \n",
    "\n",
    "Watson Machine Learning is a really powerful toolkit that gives us easy access to GPU compute resource for Machine Learning tasks in the cloud. It makes speeding up your machine learning trivial. \n",
    "\n",
    "In order to start submitting jobs to the API we need to go ahead and get the credentials though, this stops just anyone using your GPU resources and building all the AI. \n",
    "\n",
    "The [tutorial](https://github.com/ChrisParsonsDev/wml-pytorch-style-transfer) tells you how you can get your specific credentials. \n",
    "\n",
    "Make sure you call them `wml_credentials` as we did, just to keep everything working properly. \n",
    "\n",
    "Finally this code cell will configure an instance of the API called `client` that we'll set up for our PyTorch style transfer workload. \n",
    "\n",
    "Once you've replaced the placeholder below with your details, run the code cell below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/DSX-Python35/lib/python3.5/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Watson Machine Learning credentials loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-01-10 10:47:33,593 - watson_machine_learning_client.metanames - WARNING - 'AUTHOR_EMAIL' meta prop is deprecated. It will be ignored.\n",
      "2019-01-10 10:48:07,466 - watson_machine_learning_client.metanames - WARNING - 'AUTHOR_EMAIL' meta prop is deprecated. It will be ignored.\n"
     ]
    }
   ],
   "source": [
    "# Insert your Watson Machine Learning credentials here!\n",
    "\n",
    "from watson_machine_learning_client import WatsonMachineLearningAPIClient\n",
    "\n",
    "wml_credentials = {\n",
    "  \"apikey\": '***',\n",
    "  \"iam_apikey_description\": '***',\n",
    "  \"iam_apikey_name\": '***',\n",
    "  \"iam_role_crn\": '***',\n",
    "  \"iam_serviceid_crn\": '***',\n",
    "  \"instance_id\": '***',\n",
    "  \"password\": '***',\n",
    "  \"url\": '***',\n",
    "  \"username\": '***'\n",
    "}\n",
    "\n",
    "client = WatsonMachineLearningAPIClient( wml_credentials )\n",
    "\n",
    "print('Watson Machine Learning credentials loaded')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Settings\n",
    "\n",
    "Alright, so we're nearly there, we've got access to our images, we can write to the CoS bucket and Watson Machine Learning is set up and ready to go. \n",
    "\n",
    "There are just a few parameters we need to tweak to make sure everything works seemlessly. We've included 3 variables in the code cell below to make this as easy as possible. \n",
    "\n",
    "**style_image_name**\n",
    "\n",
    "For style transfer to work properly we need to supply the system with two images. Firstly, our \"style\" image. This photo contains the style and patterns we want to apply to our content image. Make sure the string here **EXACTLY MATCHES** the filename for the image you uploaded earlier. \n",
    "\n",
    "Oh, and check the file exension too, otherwise we'll end up with a file not found error, which would be really embarrassing. \n",
    "\n",
    "**content_image_name**\n",
    "\n",
    "Now we've got our style image sorted it's time for the content. This is the \"subject\" we want to apply the style to. Update the variable below so the image name matches the image you uploaded to your CoS bucket earlier on. \n",
    "\n",
    "Make sure you check that file exension.. \n",
    "\n",
    "**training_iterations**\n",
    "\n",
    "This is more commonly referred to as `epochs`. Really it's the number of times the code will run through your images attempting to improve the style transfer. The higher this number, the more style we can expect to be applied to our content image. Of course, the more iterations we have, the longer it will take. \n",
    "\n",
    "We've used 2000 as a default, but feel free to experiment with this number and see what images you can create. \n",
    "\n",
    "Update the parameters in the code cell below to match your images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "style_image_name = \"style.png\"\n",
    "content_image_name = \"content.jpeg\"\n",
    "training_iterations = \"2000\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure the Watson Machine Learning API\n",
    "\n",
    "That's all the setup complete, we simply need to configure and run our job. If you execute the code cell below we'll set up the Watson Machine Learning service to run our PyTorch code. \n",
    "\n",
    "You can see the different parameters here are configuring differeng parts of the API. \n",
    "\n",
    "**FRAMEWORK_NAME**\n",
    "\n",
    "This is the Machine Learning framework we'd like to use. Watson Machine Learning supports the majority of the Open Source ml/dl frameworks like TensorFlow and PyTorch. \n",
    "\n",
    "**FRAMEWORK_VERSION**\n",
    "\n",
    "Do you want a specific version of the framework, or are you happy with the latest and greatest? You can specify that here. \n",
    "\n",
    "**RUNTIME_NAME**\n",
    "\n",
    "Python or R? That's not a debate we want to get in to, but it's all fully customisable through the API. \n",
    "\n",
    "**RUNTIME_VERSION**\n",
    "\n",
    "You can specify a specific version of the runtime to use, if you've got dependancies on a different version of Python for instance, you can manage that here. \n",
    "\n",
    "**EXECUTION_COMMAND**\n",
    "\n",
    "This is the command you'd like the Watson Machine Learning API to execute. Here we're getting the PyTorch script, and specifying the input images and training iterations as command line arguments. \n",
    "\n",
    "Run the code cell below to configure your Watson Machine Learning job. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model definition:  13709e72-d72b-401c-b9c0-657b193aa2f8\n"
     ]
    }
   ],
   "source": [
    "metadata = {\n",
    "    client.repository.DefinitionMetaNames.NAME              : \"pytorch-styletransfer-wml\",\n",
    "    client.repository.DefinitionMetaNames.AUTHOR_EMAIL      : \"****\",\n",
    "    client.repository.DefinitionMetaNames.FRAMEWORK_NAME    : \"pytorch\",\n",
    "    client.repository.DefinitionMetaNames.FRAMEWORK_VERSION : \"1.0\",\n",
    "    client.repository.DefinitionMetaNames.RUNTIME_NAME      : \"python\",\n",
    "    client.repository.DefinitionMetaNames.RUNTIME_VERSION   : \"3.5\",\n",
    "    client.repository.DefinitionMetaNames.EXECUTION_COMMAND : \"python3 ./pytorch-model/style-transfer.py --styleImageFile ${{DATA_DIR}}/{} --contentImageFile ${{DATA_DIR}}/{} --trainingIters {}\".format(style_image_name,content_image_name,training_iterations)\n",
    "}\n",
    "\n",
    "definition_details = client.repository.store_definition( \"pytorch-model.zip\", meta_props=metadata )\n",
    "definition_uid     = client.repository.get_definition_uid( definition_details )\n",
    "\n",
    "print( \"Model definition: \", definition_uid )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Watson Machine Learning Output\n",
    "\n",
    "Now we've got our model set up, we need to tell the API where to save the output. We imported the credentials earlier in this notebook, so now we simply need to pass them over to the Watson Machine Learning API. \n",
    "\n",
    "Run this code cell below to import the storage credentials. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = {\n",
    "client.training.ConfigurationMetaNames.NAME         : \"pytorch-styletransfer-wml\",\n",
    "client.training.ConfigurationMetaNames.AUTHOR_EMAIL : \"*****\",\n",
    "client.training.ConfigurationMetaNames.TRAINING_DATA_REFERENCE : {\n",
    "   \"connection\" : { \n",
    "      \"endpoint_url\"      : image_credentials['ENDPOINT'],\n",
    "      \"access_key_id\"     : cos_credentials[\"cos_hmac_keys\"][\"access_key_id\"],\n",
    "      \"secret_access_key\" : cos_credentials[\"cos_hmac_keys\"][\"secret_access_key\"]\n",
    "      },\n",
    "   \"source\" : { \n",
    "      \"bucket\" : image_credentials['BUCKET'],\n",
    "      },\n",
    "      \"type\" : \"s3\"\n",
    "   },\n",
    "client.training.ConfigurationMetaNames.TRAINING_RESULTS_REFERENCE: {\n",
    "   \"connection\" : {\n",
    "      \"endpoint_url\"      : image_credentials['ENDPOINT'],\n",
    "      \"access_key_id\"     : cos_credentials[\"cos_hmac_keys\"][\"access_key_id\"],\n",
    "      \"secret_access_key\" : cos_credentials[\"cos_hmac_keys\"][\"secret_access_key\"]\n",
    "      },\n",
    "      \"target\" : {\n",
    "         \"bucket\" : image_credentials['BUCKET'],\n",
    "      },\n",
    "      \"type\" : \"s3\"\n",
    "   }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Forrest Run\n",
    "\n",
    "We're there! Everyting is configured, we've got our images loaded in and we're good to go. \n",
    "\n",
    "Run the code cell below to submit the style transfer job to the GPU. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run_uid:  model-hqwass1f\n"
     ]
    }
   ],
   "source": [
    "run_details = client.training.run( definition_uid, meta_props=metadata )\n",
    "run_uid     = client.training.get_run_uid( run_details )\n",
    "print( \"run_uid: \", run_uid )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Status Update\n",
    "\n",
    "To view the status of your job at any time, run the code cell below. \n",
    "\n",
    "Training, on the free tier, should take approximately 6 minutes (assuming you used the defaults).\n",
    "\n",
    "You should see the status change from:\n",
    "\n",
    "1. **Pending** - we're waiting for a GPU to be free to run our job and setting up the runtime.\n",
    "2. **Running** - woohoo! We're transferring the style.\n",
    "3. **Completed** - all done! \n",
    "\n",
    "When it's all done, check out the CoS bucket to see your results. As detailed [here](https://github.com/ChrisParsonsDev/wml-pytorch-style-transfer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'current_at': '2019-01-10T11:06:21Z',\n",
       " 'message': 'training-8feH238mg: ',\n",
       " 'metrics': [],\n",
       " 'running_at': '2019-01-10T10:50:34Z',\n",
       " 'state': 'running',\n",
       " 'submitted_at': '2019-01-10T10:48:08Z'}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.training.get_status( run_uid )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
